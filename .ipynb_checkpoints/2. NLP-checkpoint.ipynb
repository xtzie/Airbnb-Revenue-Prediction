{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual and non contextual modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.360437Z",
     "start_time": "2020-03-25T15:06:29.541684Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "from nltk.corpus import stopwords       \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.520670Z",
     "start_time": "2020-03-25T15:06:31.363040Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.762563Z",
     "start_time": "2020-03-25T15:06:31.522967Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "# df[df.neighbourhood_Mission == 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pull out the text for nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.818744Z",
     "start_time": "2020-03-25T15:06:31.766294Z"
    }
   },
   "outputs": [],
   "source": [
    "fornlp = ['id', 'access', 'description', 'host_about', 'house_rules', 'interaction', 'name', 'neighborhood_overview',\n",
    "         'notes', 'space', 'summary', 'transit', 'neighbourhood_Mission']\n",
    "\n",
    "df = df[fornlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.845935Z",
     "start_time": "2020-03-25T15:06:31.823247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8587 entries, 0 to 8586\n",
      "Data columns (total 13 columns):\n",
      "id                       8587 non-null int64\n",
      "access                   5199 non-null object\n",
      "description              8533 non-null object\n",
      "host_about               6498 non-null object\n",
      "house_rules              6217 non-null object\n",
      "interaction              5752 non-null object\n",
      "name                     8587 non-null object\n",
      "neighborhood_overview    6328 non-null object\n",
      "notes                    5163 non-null object\n",
      "space                    7290 non-null object\n",
      "summary                  8367 non-null object\n",
      "transit                  5878 non-null object\n",
      "neighbourhood_Mission    8587 non-null uint8\n",
      "dtypes: int64(1), object(11), uint8(1)\n",
      "memory usage: 813.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "airbnb listing layout:  \n",
    "Summary/Description  \n",
    "The Space  \n",
    "Guest Access  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.924420Z",
     "start_time": "2020-03-25T15:06:31.848449Z"
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(value = '', inplace = True)\n",
    "\n",
    "def length(x):\n",
    "    try:\n",
    "        return len(x)\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "df['blob'] = df['description'] + df['summary'] + df['space']\n",
    "\n",
    "df['textlength'] = df.blob.map(lambda x: length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:32.155170Z",
     "start_time": "2020-03-25T15:06:31.926612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 1722.58 median 1817.0 mode 0    1879\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 192.,  289.,  412.,  549., 1000., 1706., 2074., 1478.,  880.,\n",
       "           7.]),\n",
       " array([   0.,  300.,  600.,  900., 1200., 1500., 1800., 2100., 2400.,\n",
       "        2700., 3000.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('mean', round(df.textlength.mean(), 2), 'median', df.textlength.median(),'mode', df.textlength.mode())\n",
    "plt.hist(df.textlength.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:32.268326Z",
     "start_time": "2020-03-25T15:06:32.160581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    New update: the house next door is under const...\n",
       "1    We live in a large Victorian house on a quiet ...\n",
       "2    Nice and good public transportation.  7 minute...\n",
       "3    Nice and good public transportation.  7 minute...\n",
       "4    Pls email before booking.  Interior featured i...\n",
       "5    Welcome to \"The Mission,\" the sunniest neighbo...\n",
       "6    A Unique Guest Suite! A Spacious  Art Filled  ...\n",
       "7    Please read this before you book! Second floor...\n",
       "8    A cute studio with nice street views and lots ...\n",
       "9    This property is only available with a minimum...\n",
       "Name: blob, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text_one(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "    \n",
    "    return docs.map(text_nourl).map(text_noname).map(text_nohash)\n",
    "\n",
    "df['blob'] = process_text_one(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:35.682924Z",
     "start_time": "2020-03-25T15:06:32.271366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new update the house next door is under constr...\n",
       "1    we live in large victorian house on quiet stre...\n",
       "2    nice and good public transportation minutes wa...\n",
       "3    nice and good public transportation minutes wa...\n",
       "4    pls email before booking interior featured in ...\n",
       "5    welcome to the mission the sunniest neighborho...\n",
       "6    a unique guest suite spacious art filled space...\n",
       "7    please read this before you book second floor ...\n",
       "8    a cute studio with nice street views and lots ...\n",
       "9    this property is only available with minimum s...\n",
       "Name: blob, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nospaces).map(text_single)\n",
    "\n",
    "df.blob = process_text_two(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:35.690616Z",
     "start_time": "2020-03-25T15:06:35.686249Z"
    }
   },
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "#key words to add\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "\n",
    "# STOPWORDS = STOPWORDS.union(contextual_stop_words)\n",
    "\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=500, width = 800, height = 400,\n",
    "#                       contour_width=3, contour_color='steelblue', \n",
    "#                       stopwords = STOPWORDS)\n",
    "\n",
    "# x = list(df['blob'])\n",
    "# longstring = ','.join(x)\n",
    "# wordcloud.generate(longstring)\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:36.344578Z",
     "start_time": "2020-03-25T15:06:35.694059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new update house door construction possibility...\n",
       "1    live large victorian house quiet street nestle...\n",
       "2    nice good public transportation minutes walk u...\n",
       "3    nice good public transportation minutes walk u...\n",
       "4    pls email booking interior featured magazines ...\n",
       "5    welcome mission sunniest neighborhood town enj...\n",
       "6    unique guest suite spacious art filled space s...\n",
       "7    read book second floor suite fully furnished l...\n",
       "8    cute studio nice street views lots light easy ...\n",
       "9    property available minimum stay month historic...\n",
       "Name: blob, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "\n",
    "def process_text_three(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # Setting stopwords\n",
    "    contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "    STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "    clean_text = []\n",
    "    for textblob in docs:\n",
    "        new_blob = []\n",
    "        for word in textblob.split():\n",
    "            if (word not in STOPWORDS) and (word not in string.punctuation):\n",
    "                new_blob.append(word)\n",
    "        clean_text.append(' '.join(new_blob))\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "df.blob = process_text_three(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:36.351561Z",
     "start_time": "2020-03-25T15:06:36.347507Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE WITH THE CLEANING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.063286Z",
     "start_time": "2020-03-25T15:06:36.355617Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-485c2fd5a808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentitext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_but_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36msentiment_valence\u001b[0;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[1;32m    310\u001b[0m                         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special_idioms_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36m_negation_check\u001b[0;34m(valence, words_and_emoticons, start_i, i)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mwords_and_emoticons_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnegated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_and_emoticons_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1 word preceding lexicon word (w/o stopwords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mwords_and_emoticons_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnegated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_and_emoticons_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1 word preceding lexicon word (w/o stopwords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = []\n",
    "for doc in df['blob']:\n",
    "    vs = analyzer.polarity_scores(doc)\n",
    "    scores.append(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.067277Z",
     "start_time": "2020-03-25T15:06:29.601Z"
    }
   },
   "outputs": [],
   "source": [
    "df['senti_compound'] = [x['compound'] for x in scores]\n",
    "df['senti_neg'] = [x['neg'] for x in scores] \n",
    "df['senti_neu'] = [x['neu'] for x in scores] \n",
    "df['senti_pos'] = [x['pos'] for x in scores] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.073859Z",
     "start_time": "2020-03-25T15:06:29.607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['senti_compound'].describe()\n",
    "plt.hist(df.senti_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.076818Z",
     "start_time": "2020-03-25T15:06:29.612Z"
    }
   },
   "outputs": [],
   "source": [
    "0.086 + 0.786 +  0.128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.079485Z",
     "start_time": "2020-03-25T15:06:29.616Z"
    }
   },
   "outputs": [],
   "source": [
    "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.082307Z",
     "start_time": "2020-03-25T15:06:29.619Z"
    }
   },
   "outputs": [],
   "source": [
    "df['polarity'] = df.blob.map(polarity)\n",
    "plt.hist(df.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.085256Z",
     "start_time": "2020-03-25T15:06:29.624Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subjectivity'] = df.blob.map(subjectivity)\n",
    "plt.hist(df.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.087835Z",
     "start_time": "2020-03-25T15:06:29.632Z"
    }
   },
   "outputs": [],
   "source": [
    "#very very nice distributions! now do topic modelling, and be done with the NLP part!\n",
    "#LDA\n",
    "\n",
    "sentimentscores = df[['senti_neg', 'senti_pos', 'senti_neu', 'polarity', 'subjectivity', 'textlength']]\n",
    "\n",
    "sentimentscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.090723Z",
     "start_time": "2020-03-25T15:06:29.638Z"
    }
   },
   "outputs": [],
   "source": [
    "contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed', 've', 'll']\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "\n",
    "corpus = list(df['blob'])\n",
    "\n",
    "# create a CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=STOPWORDS)\n",
    "doc_word = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# instantiate nmf model\n",
    "nmf_model = NMF(20)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.093531Z",
     "start_time": "2020-03-25T15:06:29.643Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.097329Z",
     "start_time": "2020-03-25T15:06:29.648Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(nmf_model, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.100124Z",
     "start_time": "2020-03-25T15:06:29.654Z"
    }
   },
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(doc_topic)\n",
    "sentimentscores = sentimentscores.merge(topics, left_index = True, right_index = True)\n",
    "sentimentscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.104035Z",
     "start_time": "2020-03-25T15:06:29.658Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sentiscores', 'wb') as file:\n",
    "    pickle.dump(sentimentscores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:08:30.149066Z",
     "start_time": "2020-03-25T15:08:29.966050Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('dataframe', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "    \n",
    "fornlp = ['id', 'access', 'description', 'host_about', 'house_rules', 'interaction', 'name', 'neighborhood_overview',\n",
    "         'notes', 'space', 'summary', 'transit', 'neighbourhood_Mission']\n",
    "\n",
    "df = df[fornlp]\n",
    "mission = df[df.neighbourhood_Mission == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_one(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "    \n",
    "    return docs.map(text_nourl).map(text_noname).map(text_nohash)\n",
    "\n",
    "mission['generator'] = process_text_one(mission['description'])\n",
    "def process_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nospaces).map(text_single)\n",
    "\n",
    "mission['generator'] = process_text_two(mission['generator'])\n",
    "\n",
    "ontextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "\n",
    "def process_text_three(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # Setting stopwords\n",
    "    contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "    STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "    clean_text = []\n",
    "    for textblob in docs:\n",
    "        new_blob = []\n",
    "        for word in textblob.split():\n",
    "            if (word not in STOPWORDS) and (word not in string.punctuation):\n",
    "                new_blob.append(word)\n",
    "        clean_text.append(' '.join(new_blob))\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "df.blob = process_text_three(df['blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:08:38.434170Z",
     "start_time": "2020-03-25T15:08:37.209941Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
