{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual and non contextual modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:13.494708Z",
     "start_time": "2020-03-25T16:27:09.894558Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "from nltk.corpus import stopwords       \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:13.954941Z",
     "start_time": "2020-03-25T16:27:13.500522Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.762563Z",
     "start_time": "2020-03-25T15:06:31.522967Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "# df[df.neighbourhood_Mission == 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pull out the text for nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.818744Z",
     "start_time": "2020-03-25T15:06:31.766294Z"
    }
   },
   "outputs": [],
   "source": [
    "fornlp = ['id', 'access', 'description', 'host_about', 'house_rules', 'interaction', 'name', 'neighborhood_overview',\n",
    "         'notes', 'space', 'summary', 'transit', 'neighbourhood_Mission']\n",
    "\n",
    "df = df[fornlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.845935Z",
     "start_time": "2020-03-25T15:06:31.823247Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "airbnb listing layout:  \n",
    "Summary/Description  \n",
    "The Space  \n",
    "Guest Access  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:31.924420Z",
     "start_time": "2020-03-25T15:06:31.848449Z"
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(value = '', inplace = True)\n",
    "\n",
    "def length(x):\n",
    "    try:\n",
    "        return len(x)\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "df['blob'] = df['description'] + df['summary'] + df['space']\n",
    "\n",
    "df['textlength'] = df.blob.map(lambda x: length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:32.155170Z",
     "start_time": "2020-03-25T15:06:31.926612Z"
    }
   },
   "outputs": [],
   "source": [
    "print('mean', round(df.textlength.mean(), 2), 'median', df.textlength.median(),'mode', df.textlength.mode())\n",
    "plt.hist(df.textlength.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:32.268326Z",
     "start_time": "2020-03-25T15:06:32.160581Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_one(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "    \n",
    "    return docs.map(text_nourl).map(text_noname).map(text_nohash)\n",
    "\n",
    "df['blob'] = process_text_one(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:35.682924Z",
     "start_time": "2020-03-25T15:06:32.271366Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nospaces).map(text_single)\n",
    "\n",
    "df.blob = process_text_two(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:35.690616Z",
     "start_time": "2020-03-25T15:06:35.686249Z"
    }
   },
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "#key words to add\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "\n",
    "# STOPWORDS = STOPWORDS.union(contextual_stop_words)\n",
    "\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=500, width = 800, height = 400,\n",
    "#                       contour_width=3, contour_color='steelblue', \n",
    "#                       stopwords = STOPWORDS)\n",
    "\n",
    "# x = list(df['blob'])\n",
    "# longstring = ','.join(x)\n",
    "# wordcloud.generate(longstring)\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:36.344578Z",
     "start_time": "2020-03-25T15:06:35.694059Z"
    }
   },
   "outputs": [],
   "source": [
    "contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "\n",
    "def process_text_three(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # Setting stopwords\n",
    "    contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed']\n",
    "    STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "    clean_text = []\n",
    "    for textblob in docs:\n",
    "        new_blob = []\n",
    "        for word in textblob.split():\n",
    "            if (word not in STOPWORDS) and (word not in string.punctuation):\n",
    "                new_blob.append(word)\n",
    "        clean_text.append(' '.join(new_blob))\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "df.blob = process_text_three(df['blob'])\n",
    "df.blob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:36.351561Z",
     "start_time": "2020-03-25T15:06:36.347507Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE WITH THE CLEANING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.063286Z",
     "start_time": "2020-03-25T15:06:36.355617Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = []\n",
    "for doc in df['blob']:\n",
    "    vs = analyzer.polarity_scores(doc)\n",
    "    scores.append(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.067277Z",
     "start_time": "2020-03-25T15:06:29.601Z"
    }
   },
   "outputs": [],
   "source": [
    "df['senti_compound'] = [x['compound'] for x in scores]\n",
    "df['senti_neg'] = [x['neg'] for x in scores] \n",
    "df['senti_neu'] = [x['neu'] for x in scores] \n",
    "df['senti_pos'] = [x['pos'] for x in scores] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.073859Z",
     "start_time": "2020-03-25T15:06:29.607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['senti_compound'].describe()\n",
    "plt.hist(df.senti_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.076818Z",
     "start_time": "2020-03-25T15:06:29.612Z"
    }
   },
   "outputs": [],
   "source": [
    "0.086 + 0.786 +  0.128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.079485Z",
     "start_time": "2020-03-25T15:06:29.616Z"
    }
   },
   "outputs": [],
   "source": [
    "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.082307Z",
     "start_time": "2020-03-25T15:06:29.619Z"
    }
   },
   "outputs": [],
   "source": [
    "df['polarity'] = df.blob.map(polarity)\n",
    "plt.hist(df.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.085256Z",
     "start_time": "2020-03-25T15:06:29.624Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subjectivity'] = df.blob.map(subjectivity)\n",
    "plt.hist(df.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.087835Z",
     "start_time": "2020-03-25T15:06:29.632Z"
    }
   },
   "outputs": [],
   "source": [
    "#very very nice distributions! now do topic modelling, and be done with the NLP part!\n",
    "#LDA\n",
    "\n",
    "sentimentscores = df[['senti_neg', 'senti_pos', 'senti_neu', 'polarity', 'subjectivity', 'textlength']]\n",
    "\n",
    "sentimentscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.090723Z",
     "start_time": "2020-03-25T15:06:29.638Z"
    }
   },
   "outputs": [],
   "source": [
    "contextual_stop_words = ['san', 'francisco', 'golden', 'gate', 'bed', 've', 'll']\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(contextual_stop_words)\n",
    "\n",
    "corpus = list(df['blob'])\n",
    "\n",
    "# create a CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=STOPWORDS)\n",
    "doc_word = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# instantiate nmf model\n",
    "nmf_model = NMF(20)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.093531Z",
     "start_time": "2020-03-25T15:06:29.643Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.097329Z",
     "start_time": "2020-03-25T15:06:29.648Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(nmf_model, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:06:39.100124Z",
     "start_time": "2020-03-25T15:06:29.654Z"
    }
   },
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(doc_topic)\n",
    "sentimentscores = sentimentscores.merge(topics, left_index = True, right_index = True)\n",
    "sentimentscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:37.965797Z",
     "start_time": "2020-03-25T16:27:37.438643Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentimentscores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9e58a8788849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiscores'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentimentscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentimentscores' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sentiscores', 'wb') as file:\n",
    "    pickle.dump(sentimentscores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:39.926067Z",
     "start_time": "2020-03-25T16:27:39.627724Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "with open('dataframe', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "    \n",
    "fornlp = ['id', 'access', 'description', 'host_about', 'house_rules', 'interaction', 'name', 'neighborhood_overview',\n",
    "         'notes', 'space', 'summary', 'transit', 'neighbourhood_Mission']\n",
    "\n",
    "df = df[fornlp]\n",
    "df = df.fillna(value = '')\n",
    "mission = df[df.neighbourhood_Mission == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:57:44.470994Z",
     "start_time": "2020-03-25T16:57:44.071263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xtzie/anaconda3/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/home/xtzie/anaconda3/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6bc3665c909f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "def process_text_one(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "    \n",
    "    return docs.map(text_nourl).map(text_noname).map(text_nohash)\n",
    "\n",
    "mission['generator'] = process_text_one(mission['description'])\n",
    "def process_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nospaces).map(text_single)\n",
    "\n",
    "mission['generator'] = process_text_two(mission['generator'])\n",
    "    \n",
    "\n",
    "corpus = mission[mission.generator != '']['generator']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:57:58.760010Z",
     "start_time": "2020-03-25T16:57:58.751895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['come fall in love with our charming large elegant bedroom bath home while you enjoy all of the attractions cultural opportunities and beauty which has made san francisco one of the worlds premiere destinations if this writeup sounds too good to be true it almost is its our small slice of heaven in the middle of san francisco this year old brick carriage house was recently completely and meticulously restored to its original victorian charm and beauty while incorporating wealth of modern amenities the house is set back off the street behind another building and courtyard all part of this property providing secluded countrylike atmosphere while still in the heart of the city the following are some of the specific amenities and features of the house central heating granite countertops wholehouse stereo wholehouse wireless and wired highspeed internet wholehouse music and video distribution system with hundreds of movies and over cds t']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(corpus[9:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:19:07.860890Z",
     "start_time": "2020-03-25T15:19:07.844241Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:44.619110Z",
     "start_time": "2020-03-25T16:27:44.614006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Net Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Neural Net Training\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:45.968665Z",
     "start_time": "2020-03-25T16:27:45.774530Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(corpus.values)\n",
    "sequences = tokenizer.texts_to_sequences(corpus.values)\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:47.306604Z",
     "start_time": "2020-03-25T16:27:47.286872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5602"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:48.292484Z",
     "start_time": "2020-03-25T16:27:47.843118Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:27:51.144884Z",
     "start_time": "2020-03-25T16:27:50.239683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 19, 50)            280150    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5602)              565802    \n",
      "=================================================================\n",
      "Total params: 996,852\n",
      "Trainable params: 996,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:01:11.511912Z",
     "start_time": "2020-03-25T16:01:11.489530Z"
    }
   },
   "outputs": [],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:45:35.916054Z",
     "start_time": "2020-03-25T16:27:58.518636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112584 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fb738ed0a70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fb738ed0a70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "112584/112584 [==============================] - 224s 2ms/sample - loss: 6.4148 - accuracy: 0.0584\n",
      "Epoch 2/5\n",
      "112584/112584 [==============================] - 186s 2ms/sample - loss: 5.9357 - accuracy: 0.1036\n",
      "Epoch 3/5\n",
      "112584/112584 [==============================] - 174s 2ms/sample - loss: 5.5595 - accuracy: 0.1360\n",
      "Epoch 4/5\n",
      "112584/112584 [==============================] - 225s 2ms/sample - loss: 5.1969 - accuracy: 0.1621\n",
      "Epoch 5/5\n",
      "112584/112584 [==============================] - 247s 2ms/sample - loss: 4.8805 - accuracy: 0.1892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb738edf950>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(np.array(trainX), np.array(pd.get_dummies(np.array(trainy))), batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:55:49.239722Z",
     "start_time": "2020-03-25T16:55:49.219733Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list = [model]\n",
    "\n",
    "def gen(model,seq,max_len = 20):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
    "    max_len = max_len+len(tokenized_sent[0])\n",
    "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
    "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
    "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
    "    while len(tokenized_sent[0]) < max_len:\n",
    "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent[0].append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))\n",
    "\n",
    "def test_models(test_string,sequence_length= 50, model_list = model_list):\n",
    "    '''Generates output given input test_string up to sequence_length'''\n",
    "    print('Input String: ', test_string)\n",
    "    for counter,model in enumerate(model_list):\n",
    "        print(\"Model \", counter+1, \":\")\n",
    "        print(gen(model,test_string,sequence_length))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T16:59:05.984553Z",
     "start_time": "2020-03-25T16:59:03.125119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  Come fall in love with our charming large elegant bedroom bath home with\n",
      "Model  1 :\n",
      "come fall in love with our charming large elegant bedroom bath home with an extra and dryer the kitchen with an adventurers and the house is located in the mission district and the mission district is located in\n"
     ]
    }
   ],
   "source": [
    "test_models('Come fall in love with our charming large elegant bedroom bath home with', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
